{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489e5939-7479-4d7a-ae84-783a7446c08c",
   "metadata": {},
   "source": [
    "# __SPARK ASSIGNMENT 1__\n",
    "> Name : Ilias Dimos <br />\n",
    "> AM : f2822102 <br />\n",
    "> Course : Big Data Systems <br />\n",
    "> Email : f2822102@aueb.gr <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c855214-11c4-4e2a-83e2-962673a6fb9f",
   "metadata": {},
   "source": [
    "## __TASK 2__\n",
    "#### For this task you continue to work with SparkSQL. This time, you need to provide a Jupyter notebook (again using PySpark and Dataframes) that delivers the following:\n",
    "#### __Dataset import.__ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "39b553c7-3c52-4473-9662-6b779b3ebb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+--------+------------+--------------------+-------------------+---------+--------------------+--------+----------+-------------+-----------+-------------+--------------------+---------+--------------------+---------------+-----------------+----------------+--------------------+-------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------+\n",
      "|      asin|             authors|average_rating| book_id|country_code|         description|edition_information|   format|           image_url|is_ebook|      isbn|       isbn13|kindle_asin|language_code|                link|num_pages|     popular_shelves|publication_day|publication_month|publication_year|           publisher|ratings_count|              series|       similar_books|text_reviews_count|               title|title_without_series|                 url| work_id|\n",
      "+----------+--------------------+--------------+--------+------------+--------------------+-------------------+---------+--------------------+--------+----------+-------------+-----------+-------------+--------------------+---------+--------------------+---------------+-----------------+----------------+--------------------+-------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------+\n",
      "|B00NLXQ534|       [{8551671, }]|          4.12|25742454|          US|Lillian Ann Cross...|                   |         |https://s.gr-asse...|    true|          |             |           |             |https://www.goodr...|         |[{228, to-read}, ...|               |                 |                |                    |            1|                  []|[25653153, 256991...|                 1|The Switchblade M...|The Switchblade M...|https://www.goodr...|42749946|\n",
      "|          |       [{3274315, }]|          3.94|30128855|          US|Florence Dupre La...|                   |         |https://images.gr...|   false|2205073346|             |           |          fre|https://www.goodr...|         |[{2, bd}, {2, to-...|             22|                1|            2016|             Dargaud|           16|                  []|                  []|                 2|             Cruelle|             Cruelle|https://www.goodr...|50558228|\n",
      "|          |         [{37450, }]|          4.28|13571772|          US|The questions pla...|                   |Hardcover|https://images.gr...|   false|          |             |           |          eng|https://www.goodr...|      146|[{493, to-read}, ...|               |                 |            2012|Hachette Partwork...|           51|[246830, 362583, ...|[13590139, 105963...|                 5|Captain America: ...|Captain America: ...|https://www.goodr...|  102217|\n",
      "|B06XKGGSB7|[{16209952, }, {8...|          4.05|35452242|          US|The fight for Jas...|                   |         |https://s.gr-asse...|    true|          |             | B06XKGGSB7|          eng|https://www.goodr...|         |[{222, to-read}, ...|               |                 |                |                    |            6|                  []|                  []|                 1|Bounty Hunter 4/3...|Bounty Hunter 4/3...|https://www.goodr...|54276229|\n",
      "|          |[{81563, }, {8953...|          4.06|  707611|          US|These are the sto...|                   |Hardcover|https://images.gr...|   false|0930289765|9780930289768|           |        en-US|https://www.goodr...|      272|[{20, to-read}, {...|             14|               11|            1997|           DC Comics|           51|   [266759, 1096220]|                  []|                 6|Superman Archives...|Superman Archives...|https://www.goodr...|  693886|\n",
      "+----------+--------------------+--------------+--------+------------+--------------------+-------------------+---------+--------------------+--------+----------+-------------+-----------+-------------+--------------------+---------+--------------------+---------------+-----------------+----------------+--------------------+-------------+--------------------+--------------------+------------------+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#Builing the Spark session with name \"Assignment_1_TASK_2\n",
    "spark = SparkSession.builder.appName(\"Assignment_1_TASK_2\").getOrCreate()\n",
    "\n",
    "#Importing the \"books_5000.json\" file with the read.json() command.\n",
    "dataset = spark.read.json(\"books_5000.json\")\n",
    "#Showing the first 5 lines of our dataset\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b7b594-04f4-4d13-ac3c-c848bcc2ba71",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### __1) It returns the “book_id” and “title” of the book with the largest “average_rating” that its title starts with the first letter of your last name.__\n",
    "* In this part of the assignment in order to compute the disired outcome we created a SQL query via PySpark with the following properties.\n",
    "    * Create view named books.\n",
    "    * Selects the book_id , title and the average_rating columns.\n",
    "    * The columns have been selected from the books dataset.\n",
    "    * Chooses the books where their title  starts with the letter `\"D\"` or `\"d\"`.\n",
    "    * With the \"orderby\" function we ordered the outcome of the query based on  the average rating with descenting order to have the bigger values first.\n",
    "    * With the \"show(1)\" command we print the first row and at the same time the `largest` average rating.\n",
    "* The book with the largest average_rating has `Book_id` : __26025238__ , `Title` : __Dungeon Fun (Dungeon Fun #4)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "d8a06163-1585-4056-ad91-e104851919bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+--------------+\n",
      "|book_id |title                       |average_rating|\n",
      "+--------+----------------------------+--------------+\n",
      "|26025238|Dungeon Fun (Dungeon Fun #4)|5.00          |\n",
      "+--------+----------------------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.createOrReplaceTempView(\"books\")\n",
    "\n",
    "query = ''' SELECT book_id,title, average_rating\n",
    "            FROM books \n",
    "            WHERE title LIKE 'D%' or title LIKE 'd%' '''\n",
    "\n",
    "books_df_1 = spark.sql(query)\n",
    "\n",
    "books_df_1.orderBy('average_rating', ascending=False).show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49b2663-776c-4d46-917a-4150d8c6233b",
   "metadata": {},
   "source": [
    "#### __2) It returns the average “average_rating” of the books that their title starts with the *second* letter of your last name.__\n",
    " * In this part of the assignment in order to compute the disired outcome we created a SQL query via PySpark with the following characteristics.\n",
    "     * Selects and computes the average average_ratings.\n",
    "     * From the view \"books\".\n",
    "     * Chooses the books where their title starts with the letter `\"I\"` or `\"i\"`.\n",
    " * The `average` \"average rating\" of the books is : __3.955161290322582__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "727c14e6-d0b0-4e5d-ba34-c9684835a399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|          average|\n",
      "+-----------------+\n",
      "|3.955161290322582|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.createOrReplaceTempView(\"books\")\n",
    "\n",
    "query = ''' SELECT avg(average_rating) as average \n",
    "            FROM books \n",
    "            WHERE title LIKE 'I%' or title LIKE 'i%' '''\n",
    "\n",
    "books_df_2= spark.sql(query)\n",
    "\n",
    "books_df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a735882-95e4-455c-a9d7-95915d809870",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### __3) It returns the “book_id” and “title” of the Paperback book with the most pages, when only books with title starting with the *third* letter of your last name are considered.__\n",
    " * In this part of the assignment in order to compute the disired outcome we created a SQL query via PySpark with the following characteristics.\n",
    "    * Creates a new dataframe named `dataset_paperback` that contains the books having `Paperback` format.\n",
    "      * The Paperback format has 6 levels,based on the last query of the `Τask 1`, the : `Paperback,paperback,Mass Market Paperback,Trade Paperback,Paperback comic book,Paperback Manga`.\n",
    "    * Turns the data type of the \"num_page\" from `String` to `Interger` with the `CAST` SQL function and saves it with the name `pages`.\n",
    "    * Chooses the titles starting with letter `\"M\"` or `\"m\"` .\n",
    "    * In order to print the pages in descenting order we used the `ORDER BY ... desc ` SQL function.\n",
    "    * Because in the `SELECT` argument we print the pages of each book as extra , and the `task 3` only asks for the `book_id` and the `title` , with the command `.drop(\"pages\")` we delete this extra column in order to print the desired outcome.\n",
    " * The book which is Paperback , has the most pages and is starting with the letter \"M\" or \"m\" has `book_id` : __28687715__ and `title` : __Mafalda: Todas las tiras.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3541e46c-4409-444d-8f81-be2d582a0c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|book_id |title                   |\n",
      "+--------+------------------------+\n",
      "|28687715|Mafalda: Todas las tiras|\n",
      "+--------+------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# We select the rows that their format is 'Paperback'\n",
    "dataset_paperback=dataset.filter(col(\"format\").isin('Paperback','paperback','Mass Market Paperback','Trade Paperback','Paperback comic book','Paperback Manga'))\n",
    "dataset_paperback.createOrReplaceTempView(\"paperback_books\")\n",
    "query = ''' SELECT CAST(num_pages AS integer) as pages , book_id, title\n",
    "            FROM paperback_books \n",
    "            WHERE title LIKE 'M%' or title LIKE 'm%' \n",
    "            ORDER BY pages desc'''\n",
    "books_df_3 = spark.sql(query)\n",
    "books_df_3.drop(\"pages\").show(1,truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
